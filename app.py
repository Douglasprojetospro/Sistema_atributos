import streamlit as st
import pandas as pd
from io import BytesIO
import re
import tempfile
import os
from pathlib import Path

# Configura√ß√µes para melhor performance com arquivos grandes
st.set_page_config(page_title="Processador de Planilhas - Otimizado", page_icon="üìä", layout="wide")

# Configura√ß√µes do pandas para melhor performance
pd.set_option('mode.chained_assignment', None)

st.title("üìä Processador de Planilhas - Otimizado para Grandes Arquivos")
st.markdown("---")

# Colunas para os templates
col1, col2 = st.columns(2)

with col1:
    st.subheader("üìã Modelos para Download")
    
    # Template de dados
    data_template = pd.DataFrame({
        'ID': [1414, 2525],
        'Descri√ß√£o': ['Ventilador de teto 110 amarelo biv', 'Lumin√°ria LED 220v branca']
    })
    
    # Template de configura√ß√µes
    config_template = pd.DataFrame({
        'Atributo': ['Voltagem', 'Voltagem', 'Voltagem', 'Cor', 'Cor'],
        'Varia√ß√£o': ['110v', '220v', 'Bivolt', 'Amarelo', 'Branca'],
        'Padr√£o de reconhecimento': ['110,110v,127', '220,220v,227', 'bivolt,biv', 'amarelo,yellow', 'branca,white']
    })

    # Fun√ß√£o para converter DataFrame para Excel
    def to_excel(df):
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False, sheet_name='Sheet1')
        return output.getvalue()

    st.download_button(
        "üì• Baixar modelo de dados", 
        to_excel(data_template), 
        file_name="modelo_dados.xlsx",
        help="Modelo da planilha com colunas ID e Descri√ß√£o"
    )
    
    st.download_button(
        "üì• Baixar modelo de configura√ß√µes", 
        to_excel(config_template), 
        file_name="modelo_config.xlsx",
        help="Modelo da planilha com colunas Atributo, Varia√ß√£o e Padr√£o de reconhecimento"
    )

st.markdown("---")

# Upload de arquivos com op√ß√µes para grandes arquivos
st.subheader("üì§ Upload dos Arquivos (Otimizado para Grandes Arquivos)")

upload_col1, upload_col2 = st.columns(2)

with upload_col1:
    st.info("üí° **Dica para arquivos grandes:** Divida sua planilha de dados em partes menores se poss√≠vel.")
    data_file = st.file_uploader("Planilha de dados (XLSX)", type="xlsx", key="data_upload")

with upload_col2:
    st.info("üí° **Arquivo de configura√ß√µes:** Geralmente √© pequeno, sem problemas de tamanho.")
    config_file = st.file_uploader("Planilha de configura√ß√µes (XLSX)", type="xlsx", key="config_upload")

# Fun√ß√£o otimizada para processamento em lotes
def processar_em_lotes(data_df, config_df, tamanho_lote=1000):
    """
    Processa os dados em lotes para economizar mem√≥ria
    """
    # Pr√©-processar configura√ß√µes
    config_groups = config_df.groupby('Atributo')
    config_dict = {}
    
    for attr, group in config_groups:
        config_dict[attr] = []
        for _, row in group.iterrows():
            patterns = [p.strip().lower() for p in str(row['Padr√£o de reconhecimento']).split(',')]
            config_dict[attr].append({
                'variation': str(row['Varia√ß√£o']),
                'patterns': patterns
            })
    
    # Processar em lotes
    resultados = []
    total_linhas = len(data_df)
    
    for i in range(0, total_linhas, tamanho_lote):
        lote = data_df.iloc[i:i + tamanho_lote].copy()
        
        for attr, configs in config_dict.items():
            coluna_resultado = []
            
            for _, linha in lote.iterrows():
                descricao = str(linha['Descri√ß√£o']).lower()
                variacoes_encontradas = []
                
                for config in configs:
                    for pattern in config['patterns']:
                        if pattern and re.search(r'\b' + re.escape(pattern) + r'\b', descricao):
                            if config['variation'] not in variacoes_encontradas:
                                variacoes_encontradas.append(config['variation'])
                            break
                
                coluna_resultado.append(', '.join(variacoes_encontradas) if variacoes_encontradas else '')
            
            lote[attr] = coluna_resultado
        
        resultados.append(lote)
        
        # Atualizar progresso
        progresso = min((i + len(lote)) / total_linhas, 1.0)
        yield progresso, lote
    
    yield 1.0, pd.concat(resultados, ignore_index=True) if resultados else pd.DataFrame()

# Fun√ß√£o para ler arquivo de forma eficiente
def ler_arquivo_eficiente(arquivo):
    """
    L√™ arquivo Excel de forma otimizada
    """
    try:
        # Tentar ler apenas as colunas necess√°rias
        if arquivo.name.endswith('.xlsx'):
            # Ler metadados para verificar tamanho
            temp_dir = tempfile.mkdtemp()
            temp_path = os.path.join(temp_dir, arquivo.name)
            
            with open(temp_path, 'wb') as f:
                f.write(arquivo.getvalue())
            
            # Verificar tamanho do arquivo
            tamanho_mb = os.path.getsize(temp_path) / (1024 * 1024)
            st.info(f"üìÅ Tamanho do arquivo: {tamanho_mb:.2f} MB")
            
            # Estrat√©gias diferentes baseadas no tamanho
            if tamanho_mb > 50:
                st.warning("‚ö° Arquivo grande detectado. Usando modo de leitura otimizado...")
                # Ler em chunks para arquivos muito grandes
                xl = pd.ExcelFile(temp_path)
                sheets = xl.sheet_names
                
                # Ler primeira linha para verificar colunas
                primeira_linha = pd.read_excel(temp_path, nrows=1)
                
                # Ler dados com otimiza√ß√µes
                df = pd.read_excel(
                    temp_path,
                    usecols=['ID', 'Descri√ß√£o'] if all(col in primeira_linha.columns for col in ['ID', 'Descri√ß√£o']) else None,
                    dtype={'ID': 'str', 'Descri√ß√£o': 'str'},
                    engine='openpyxl'
                )
                
            else:
                # Leitura normal para arquivos menores
                df = pd.read_excel(temp_path, engine='openpyxl')
            
            # Limpar arquivo tempor√°rio
            os.remove(temp_path)
            os.rmdir(temp_dir)
            
            return df
            
    except Exception as e:
        st.error(f"Erro ao ler arquivo: {str(e)}")
        return None

if data_file and config_file:
    try:
        # Mostrar informa√ß√µes de processamento
        st.subheader("‚öôÔ∏è Configura√ß√µes de Processamento")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            usar_lotes = st.checkbox("Usar processamento em lotes", value=True, 
                                   help="Recomendado para arquivos grandes")
        
        with col2:
            tamanho_lote = st.selectbox("Tamanho do lote", 
                                      [500, 1000, 2000, 5000], 
                                      index=1,
                                      help="N√∫mero de linhas processadas por vez")
        
        with col3:
            mostrar_preview = st.checkbox("Mostrar preview", value=True,
                                        help="Mostrar amostra dos dados")
        
        # Ler arquivos
        st.subheader("üìñ Lendo Arquivos...")
        
        progress_bar_leit = st.progress(0)
        
        # Ler arquivo de configura√ß√µes (geralmente pequeno)
        config_df = pd.read_excel(config_file)
        progress_bar_leit.progress(50)
        
        # Ler arquivo de dados com estrat√©gia otimizada
        data_df = ler_arquivo_eficiente(data_file)
        progress_bar_leit.progress(100)
        
        if data_df is None:
            st.error("‚ùå Erro ao ler arquivo de dados")
            st.stop()
        
        # Validar colunas
        required_data_cols = ['ID', 'Descri√ß√£o']
        required_config_cols = ['Atributo', 'Varia√ß√£o', 'Padr√£o de reconhecimento']
        
        if not all(col in data_df.columns for col in required_data_cols):
            st.error(f"‚ùå Planilha de dados deve conter as colunas: {required_data_cols}")
            st.stop()
            
        if not all(col in config_df.columns for col in required_config_cols):
            st.error(f"‚ùå Planilha de configura√ß√µes deve conter as colunas: {required_config_cols}")
            st.stop()
        
        # Mostrar estat√≠sticas
        st.subheader("üìä Estat√≠sticas dos Dados")
        info_col1, info_col2, info_col3, info_col4 = st.columns(4)
        
        with info_col1:
            st.metric("Linhas de dados", len(data_df))
        
        with info_col2:
            st.metric("Colunas de dados", len(data_df.columns))
        
        with info_col3:
            st.metric("Atributos configurados", len(config_df['Atributo'].unique()))
        
        with info_col4:
            tamanho_memoria = data_df.memory_usage(deep=True).sum() / (1024 * 1024)
            st.metric("Uso de mem√≥ria (MB)", f"{tamanho_memoria:.1f}")
        
        if mostrar_preview:
            st.subheader("üëÄ Preview dos Dados")
            
            preview_col1, preview_col2 = st.columns(2)
            
            with preview_col1:
                st.write("**Planilha de Dados** (primeiras 5 linhas)")
                st.dataframe(data_df.head(), use_container_width=True)
            
            with preview_col2:
                st.write("**Planilha de Configura√ß√µes**")
                st.dataframe(config_df.head(10), use_container_width=True)
        
        # Processamento
        st.subheader("‚öôÔ∏è Processando Dados...")
        
        if usar_lotes and len(data_df) > 1000:
            st.info(f"üîß Processando em lotes de {tamanho_lote} linhas...")
            
            # Criar placeholder para resultados parciais
            resultado_placeholder = st.empty()
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            resultados_parciais = []
            
            for progresso, lote_processado in processar_em_lotes(data_df, config_df, tamanho_lote):
                progress_bar.progress(progresso)
                status_text.text(f"Progresso: {progresso*100:.1f}% - Processadas {min((progresso * len(data_df)), len(data_df)):.0f} de {len(data_df)} linhas")
                
                if progresso < 1.0:
                    resultados_parciais.append(lote_processado)
                else:
                    result_df = lote_processado  # √öltimo yield cont√©m o DataFrame completo
            
            status_text.text("‚úÖ Processamento conclu√≠do!")
            
        else:
            # Processamento direto para arquivos menores
            st.info("üîß Processamento direto (arquivo pequeno)...")
            progress_bar = st.progress(0)
            
            # Processar tudo de uma vez
            config_groups = config_df.groupby('Atributo')
            result_df = data_df.copy()
            
            total_attrs = len(config_groups)
            
            for i, (attr, group) in enumerate(config_groups):
                progress_bar.progress(i / total_attrs)
                
                variations_list = []
                for _, data_row in data_df.iterrows():
                    descricao = str(data_row['Descri√ß√£o']).lower()
                    matched_variations = []
                    
                    for _, config_row in group.iterrows():
                        patterns = [p.strip().lower() for p in str(config_row['Padr√£o de reconhecimento']).split(',')]
                        variation = str(config_row['Varia√ß√£o'])
                        
                        for pattern in patterns:
                            if pattern and re.search(r'\b' + re.escape(pattern) + r'\b', descricao):
                                if variation not in matched_variations:
                                    matched_variations.append(variation)
                                break
                    
                    variations_list.append(', '.join(matched_variations) if matched_variations else '')
                
                result_df[attr] = variations_list
            
            progress_bar.progress(1.0)
        
        # Resultado final
        st.subheader("üìä Resultado Final")
        
        # Mostrar apenas uma amostra se for muito grande
        if len(result_df) > 1000:
            st.warning(f"üìã Mostrando as primeiras 1000 linhas de {len(result_df)} total")
            st.dataframe(result_df.head(1000), use_container_width=True)
        else:
            st.dataframe(result_df, use_container_width=True)
        
        # Estat√≠sticas finais
        st.subheader("üìà Estat√≠sticas do Processamento")
        stat_col1, stat_col2, stat_col3 = st.columns(3)
        
        with stat_col1:
            total_matches = sum([result_df[attr].str.count(',').sum() + result_df[attr].ne('').sum() 
                               for attr in config_df['Atributo'].unique()])
            st.metric("Total de Correspond√™ncias", int(total_matches))
        
        with stat_col2:
            atributos_com_match = sum([1 for attr in config_df['Atributo'].unique() 
                                     if result_df[attr].ne('').any()])
            st.metric("Atributos com Match", atributos_com_match)
        
        with stat_col3:
            linhas_com_match = result_df[[attr for attr in config_df['Atributo'].unique()]].ne('').any(axis=1).sum()
            st.metric("Linhas com Match", linhas_com_match)
        
        # Download do resultado
        st.subheader("üì• Download do Resultado")
        
        st.warning("üí° **Aten√ß√£o:** Para arquivos muito grandes, o download pode demorar.")
        
        # Op√ß√£o de download em partes para arquivos muito grandes
        if len(result_df) > 50000:
            st.info("üìÅ Arquivo muito grande. Recomendamos dividir o download:")
            
            partes = (len(result_df) // 50000) + 1
            for i in range(partes):
                inicio = i * 50000
                fim = min((i + 1) * 50000, len(result_df))
                parte_df = result_df.iloc[inicio:fim]
                
                parte_excel = to_excel(parte_df)
                st.download_button(
                    f"üíæ Baixar Parte {i+1} (linhas {inicio+1}-{fim})", 
                    parte_excel, 
                    file_name=f"relatorio_parte_{i+1}.xlsx",
                    help=f"Parte {i+1} do relat√≥rio"
                )
        else:
            # Download √∫nico
            result_excel = to_excel(result_df)
            
            st.download_button(
                "üíæ Baixar Relat√≥rio Completo", 
                result_excel, 
                file_name="relatorio_final.xlsx",
                help="Planilha com os resultados do processamento",
                type="primary"
            )
        
        st.success("‚úÖ Processamento conclu√≠do com sucesso!")
        
    except Exception as e:
        st.error(f"‚ùå Erro durante o processamento: {str(e)}")
        st.info("üí° Para arquivos muito grandes, tente dividi-los em partes menores.")

else:
    st.info("üëÜ Fa√ßa o upload de ambas as planilhas para iniciar o processamento.")

# Estrat√©gias para arquivos muito grandes
with st.expander("üöÄ Estrat√©gias para Arquivos Muito Grandes (500MB+)"):
    st.markdown("""
    ### Se seus arquivos forem maiores que 500MB:
    
    **1. Divida os arquivos de dados:**
    - Separe em m√∫ltiplos arquivos de ~100MB cada
    - Processe um por um
    - Combine os resultados depois
    
    **2. Use um servidor com mais recursos:**
    - Aumente a mem√≥ria RAM dispon√≠vel
    - Use inst√¢ncias com melhor processamento
    
    **3. Otimize suas planilhas:**
    - Remova colunas desnecess√°rias
    - Use compacta√ß√£o ZIP nos arquivos XLSX
    - Converta para CSV (menor tamanho)
    
    **4. Processamento em nuvem:**
    - Use servi√ßos como AWS, Google Cloud
    - Processe em m√°quinas mais potentes
    """)

with st.expander("‚ÑπÔ∏è Instru√ß√µes de Uso"):
    st.markdown("""
    ### Modo Otimizado para Grandes Arquivos:
    
    1. **Para arquivos at√© 200MB:** Processamento normal
    2. **Para arquivos 200MB-500MB:** Use o modo de lotes
    3. **Para arquivos 500MB+:** Divida em partes menores
    
    ### Dicas de Performance:
    - Marque "Usar processamento em lotes" para arquivos grandes
    - Ajuste o tamanho do lote baseado na sua mem√≥ria
    - Para arquivos enormes, divida antes do upload
    """)
